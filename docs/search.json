[
  {
    "objectID": "week10.html",
    "href": "week10.html",
    "title": "Week 10 | TBA",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "Week 7 | TBA",
    "section": "",
    "text": "Class Details\n\n\n\nüìÖ Date: 02 May, 2025\n‚è∞ Time: 14:30h - 16:30h\nüìñ Synopsis: Evaluation of the experimental design figure, and providing feedback on best practices for reproducible manuscript writing.\n\n\n\nDone in ClassTo Do at Home\n\n\n\nReviewed the experimental design figure for clarity, completeness, and logical flow.\nAssessed whether the figure effectively communicates the study‚Äôs objectives, data collection timeline, and key variables.\nIdentified areas for improvement in visual presentation and labeling.\nDiscussed common pitfalls and best practices in designing figures for clarity and aesthetics.\nProvided feedback on how to align the figure with the narrative of the manuscript.\nShared guidelines for reproducible manuscript writing, emphasizing the importance of collaborative documents, narrative structure, version control, and appropriate citation of tools.\n\n\n\n\nContinue writing the Scientific Data manuscript that describes the dataset analyzed throughout the course.\nClean up and reformat the data analysis scripts, following the best practices discussed in earlier classes.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "sandbox/week1_tabs.html",
    "href": "sandbox/week1_tabs.html",
    "title": "Week 1 | 1. System setup",
    "section": "",
    "text": "Prerequisites | Mandatory:\n\nYou should bring your own laptop. This course teaches essential data analysis skills, including setting up and managing your computing environment. Bringing your own laptop ensures you can apply what you learn, practice continuously, and integrate it into your research.\n\nIf you do not have one, an RStudio Server account can be created.\n\nYou must have administrator privileges to install software in your computer;\n\n\n\nHow to setup your system for reproducible data analysis\nReproducibility is a cornerstone of reliable data analysis, ensuring that results can be consistently replicated and built upon.\nSetting up a system for reproducible data analysis requires careful planning and adherence to best practices. Bellow is an overview of the major steps a researcher should take into consideration.\n\n\n1|Define Objectives2| Version Control3|Data Management4|Automated Workflow5|Literate Programming6|Ensure Reproducibility7|Collaboration & Transparency8|Archive & Maintain\n\n\nDefine Objectives and Requirements\n\nClearly state the research question and expected outputs.\nIdentify data sources and formats (structured/unstructured, CSV, VCF, JSON, etc.).\nAssess computational resources (local vs.¬†cloud-based, access to high-performance computing).\n\n\n\nChoose a Version-Controlled Environment\n\nUse Git with GitHub, GitLab, or Bitbucket to track code changes.\nConsider RStudio Projects (for R) or Jupyter Notebooks (for Python) for self-contained workflows.\nUse Docker or Conda for managing software dependencies.\n\n\n\n\nEstablish raw data integrity: keep an unaltered version of raw data.\nUse structured data storage (CSV, Parquet, SQL databases).\nDocument data with metadata (e.g., data dictionaries, README files).\n\n\n\nDevelop a Modular and Scripted Workflow\n\nUse scripts instead of manual steps (e.g., R scripts, Python scripts).\nImplement functions and reusable code to avoid redundancy.\nOrganize code using directory structures (e.g., data/, scripts/, results/).\nAutomate tasks using Snakemake (best for Python), or Nextflow (best for R).\n\n\n\nAutomate Analysis and Reporting\n\nUse RMarkdown, Quarto, or Jupyter Notebooks for dynamic reports.\nMaintain clean and annotated code with comments and structured logic.\nImplement parameterized reports to adapt analyses to new datasets.\n\n\n\n\nTrack software versions using renv (R), Conda (Python), or Docker.\nSpecify package dependencies in requirements.txt (Python) or DESCRIPTION (R).\nAvoid using relative paths; use project-root-based references instead.\nStore intermediate outputs systematically (results/, figures/).\nAvoid manually modifying output files; regenerate from scripts.\n\n\n\n\nShare code via public repositories (GitHub, OSF, Zenodo).\nWrite clear README files and usage instructions.\nUse open formats (e.g., CSV over Excel, Markdown over Word).\n\n\n\nArchive and Maintain Reproducibility\n\nProvide a complete computational environment snapshot (e.g., sessionInfo() in R).\nCreate DOIs for code and datasets using Zenodo or Figshare.\nDefine a long-term data storage plan (institutional repositories, cloud backups).\n\n\n\n\n\n\nTools used for this course\nIn this course, we will use R and RStudio for data analysis, and Git and GitHub for version control:\n\nR is a powerful programming language designed for statistical computing and data analysis.\n\nRStudio is an integrated development environment (IDE) that provides a user-friendly interface for writing, debugging, and managing R code.\n\nGit is a version control system that tracks changes in code, allowing for efficient collaboration and history management.\n\nGitHub is a cloud-based platform for hosting Git repositories, enabling code sharing, collaboration, and version tracking across teams.\n\nBy combining these tools, researchers can streamline their workflows, ensure reproducibility, and foster collaboration and efficiency in data-driven projects.\n\n\nInstall software\n\nFirst install R: https://cran.r-project.org/\nThen install RStudio: https://posit.co/download/rstudio-desktop/\nInstall git: https://git-scm.com/downloads\n- Detailed instructions for each Operating System (Linux, Mac, Windows) here.\nCreate an account in GitHub: https://docs.github.com/en/get-started/start-your-journey/creating-an-account-on-github\n\n\n\nNext steps\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week8.html",
    "href": "week8.html",
    "title": "Week 8 | TBA",
    "section": "",
    "text": "Class Details\n\n\n\nüìÖ Date: 14 May, 2025\n‚è∞ Time: 14:00h - 16:00h\nüìñ Synopsis: TBA\n\n\n\nDone in ClassTo Do at Home\n\n\n \n\n\nParticular tasks for BB\n\nRefactor the data analysis script by splitting the current single script into two separate scripts: one for Descriptive Analysis and one for Modeling.\nApply the following changes to the data analysis scripts:\n\nAdd the site variable to the tidy dataset.\nExclude the id variable from the MCA calculation.\nColor the PCA plots using relevant categorical variables to help identify groupings.\nMove the PCA visualization with variable vectors to the modeling script.\nCombine the individual histograms into a single multi-panel figure.\nRemove the printing of descriptive statistics for each variable separately; keep only the summary table with all variables described.\n\nDraft the Scientific Data descriptor manuscript: Prepare a clean first draft to share with the tutor for feedback.\nPrepare final questions or suggestions related to reproducible data analysis for discussion.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week9.html",
    "href": "week9.html",
    "title": "Week 9 | TBA",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contact_me.html",
    "href": "contact_me.html",
    "title": "Contact me",
    "section": "",
    "text": "Feel free to reach out if you need assistance with these training materials, or just to say hi!\n\nLoading‚Ä¶\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "Week 6 | 4. R Code review & Experimental Design Figure",
    "section": "",
    "text": "Class Details\n\n\n\nüìÖ Date: 15 & 16 April, 2025\n‚è∞ Time: 3h (with each student)\nüìñ Synopsis: Individual sessions focused on reviewing R code and advising on script modularization, function creation, and best practices for data analysis. Critical reviewing of experimental design figure.\n\n\n\nDone in ClassTo Do at Home\n\n\nCode Structure, Reproducibility, and Experimental Design\n\nReviewed and critically evaluated each student‚Äôs experimental design figure from the previous week‚Äôs assignment. Feedback focused on improving clarity, completeness, and informativeness. Students were asked to revise the figure by incorporating missing elements and ensuring alignment with their analytical goals.\nProvided guidance on transitioning from monolithic scripts to a modular workflow:\n\nSeparate scripts by purpose: data import, cleaning, analysis (per question, per model, per dataset, etc), visualization.\nName and organize scripts consistently to reflect their role in the pipeline.\n\nDiscussed when and how to define custom functions:\n\nIdentify repeated or logically distinct code blocks.\nEnsure functions are self-contained, well-documented, and generalizable.\n\nOutlined best practices for reproducible data analysis:\n\nUse version control (e.g., Git) to track changes and collaborate efficiently.\nStructure the project directory with clear input/output folders and relative paths.\nDocument all steps using literate programming tools (e.g., R Markdown).\nAvoid hardcoding values; use parameters and explicit variabkes at the beginning of the script.\n\nEmphasized the importance of treating each analysis project as a reproducible research product that can be understood and re-executed by your ‚Äúfuture self‚Äù, and others, including collaborators or reviewers.\n\n\n\nFor next class\nPlease work on the following tasks to prepare for our next discussion and continue developing your project:\n\nUpdate the experimental design figure\nRevise your experimental design figure based on our latest discussions, making sure it accurately reflects your current project setup. In the next class, we will revisit it together for further refinements and identify any final adjustments. Be mindful of where and how you save this figure‚Äîlike your code, figures should be reproducible and version-controlled. Save yourself future frustration by staying organized now.\nStart drafting your scientific data manuscript\nBegin shaping your manuscript by drafting the title, introduction/background, methods, and technical validation sections‚Äîthese form the foundation of your narrative. Leave the abstract for last, as it is easier to write once the full story is clear.\n\nBe your own toughest reviewer: assess your writing for clarity, coherence, and completeness.\n\nUse the provided manuscript template and study other published examples in your field to understand the tone and structure expected.\n\nDecide where and how you‚Äôll store and edit your manuscript: Is it a collaborative document (e.g.¬†Google docs)? Which software will you use to write it? How abaout version control and backup of the document?\nPlan how you will manage references: which citation software will you use, and is your library updated for seamless integration?\n\nTaking the time now to address these workflow details will ensure a smoother and more reproducible writing process later.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar",
    "section": "",
    "text": "The scheduled classes for this curse in 2025 are:\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Techniques for Reproducible Data Analysis",
    "section": "",
    "text": "Last update | April 2025 | https://phdcomics.com/\n\n\n\n\n\n\nThe Reproducible Data Analysis (RDA) course at University do Algarve focuses on methods for ensuring transparency and consistency in data analysis. Topics include the data analysis cycle, version control with Git, literate programming using R notebooks, and best practices for reproducibility.\nCourse materials and resources are available here to support learning and practice.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "Week 2 | 2. R Intro & Tidy data",
    "section": "",
    "text": "Class Details\n\n\n\nüìÖ Date: 13 March, 2025\n‚è∞ Time: 09:00h - 11:00h\nüìñ Synopsis: Brief introduction to R, and Introduction to tidy data"
  },
  {
    "objectID": "week2.html#r4ab-hands-on-tutorial",
    "href": "week2.html#r4ab-hands-on-tutorial",
    "title": "Week 2 | 2. R Intro & Tidy data",
    "section": "R4AB | Hands on tutorial",
    "text": "R4AB | Hands on tutorial\nThis mini hands-on tutorial serves as an introduction to basic R, covering the following topics:\n\nOnline sources of information about R;\nPackages, Documentation and Help;\nBasics and syntax of R;\nMain R data structures: Vectors, Matrices, Data frames, Lists, and Factors;\nBrief intro to R control-flow via Loops and Conditionals;\nBrief description of function declaration;\nListing of some of the most commonly used built-in functions in R.\n\n\n\nTutorial organization\nThis protocol is divided into 7 parts, each one identified by a Title, Maximum execution time (in parenthesis), a brief Task description and the R commands to be executed. These will always be inside grey text boxes, with the font colored according to the R syntax highlighting.\n\n\n\nOnline sources and other useful Bibliography\n\nWebsites\n\nR Project (The developers of R)\nQuick-R (Roadmap and R code to quickly use R)\nCookbook for R (R code ‚Äúrecipes‚Äù)\nBioconductor workflows (R code for pipelines of genomic analyses)\nIntroduction to Data Science (Free online book from Rafael A. Irizarry, 2020)\nAdvanced R (If you want to learn R from a programmers perspective)\n\nBooks\n\nIntroductory Statistics with R (Springer, Dalgaard, 2008)\nA first course in statistical programming with R (CUP, Braun and Murdoch, 2016)\nComputational Genome Analysis: An Introduction (Springer, Deonier, Tavar√© and Waterman, 2005)\nR programming for Bioinformatics (CRC Press, Gentleman, 2008)\nR for Data Science: Import, Tidy, Transform, Visualize, and Model Data (O‚ÄôReilly, Wickham and Grolemund, 2017) (for advanced users)\n\n\n\n\nBasics\n\nGeneral notes (about R and RStudio)\n\nR is case sensitive - be aware of capital letters (b is different from B).\nAll R code lines starting with the # (hash) sign are interpreted as comments, and therefore not evaluated.\n\n\n\n# This is a comment\n# 3 + 4   # this code is not evaluated, so and it does not print any result\n2 + 3     # the code before the hash sign is evaluated, so it prints the result (value 5)\n\n[1] 5\n\n\n\nExpressions in R are evaluated from the innermost parenthesis toward the outermost one (following proper mathematical rules).\n\n\n# Example with parenthesis:\n((2+2)/2)-2\n\n[1] 0\n\n# Without parenthesis:\n2+2/2-2\n\n[1] 1\n\n\n\nSpaces matter in variable names ‚Äî use a dot or underscore to create longer names to make the variables more descriptive, e.g.¬†my.variable_name.\n\nSpaces between variables and operators do not matter: 3+2 is the same as 3 + 2, and function (arg1 , arg2) is the same as function(arg1,arg2).\n\nIf you want to write 2 expressions/commands in the same line, you have to separate them by a ; (semi-colon)\n\n\n#Example:\n3 + 2 ; 5 + 1  \n\n[1] 5\n\n\n[1] 6\n\n\n\nMore recent versions of RStudio auto-complete your commands by showing you possible alternatives as soon as you type 3 consecutive characters, however, if you want to see the options for less than 3 chars, just press tab to display available options. Tip: Use auto-complete as much as possible to avoid typing mistakes.\nThere are 4 main vector data types: Logical (TRUE or FALSE); Numeric (e.g.¬†1,2,3‚Ä¶); Character (e.g.¬†‚Äúu‚Äù, ‚Äúalg‚Äù, ‚Äúarve‚Äù) and Complex (e.g.¬†3+2i)\nVectors are ordered sets of elements. In R vectors are 1-based, i.e.¬†the first index position is number 1 (as opposed to other programming languages whose indexes start at zero).\nR objects can be divided in two main groups: Functions and Data-related objects. Functions receive arguments inside circular brackets ( ) and objects receive arguments inside square brackets [ ]:\nfunction (arguments)\ndata.object [arguments]\n\n\n\n\nStart/Quit RStudio\nRStudio can be opened by double-clicking its icon.\nThe R environment is controlled by hidden files (files that start with a .) in the start-up directory: .RData, .Rhistory and .Rprofile (optional).\n\n.RData is a file containing all the objects, data, and functions created during a work-session. This file can then be loaded for future work without requiring the re-computation of the analysis. (Note: it can potentially be a very large file);\n.Rhistory saves all commands that have been typed during the R session;\n.Rprofile useful for advanced users to customize RStudio behavior.\n\nIt is always good practice to rename these files:\n# DO NOT RUN\nsave.image (file=\"myProjectName.RData\")\nsavehistory (file=\"myProjectName.Rhistory\")\n\nTo quit R, just close RStudio or use the q () function, and you will be asked if you want to save the workspace image (i.e.¬†the .RData file):\nq()\nSave workspace image to ~/path/to/your/working/directory/.RData? [y/n/c]:\nBy typing y (yes), then the entire R workspace will be written to the .RData file (which can be very large). Often it is sufficient to just save an analysis script (i.e.¬†a reproducible protocol) in an R source file. This way, one can quickly regenerate all data sets and objects for future analysis. The .RData file is particularly useful to save the results from analyses that require a long time to compute, and to keep checkpoints of your analysis pipeline.\n\n\n\nPackage repositories\nIn R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others. These packages are stored online from which they can be easily retrieved and installed on your computer (R packages by Hadley Wickham). There are 2 main R repositories:\n\nThe Comprehensive R Archive Network - CRAN (14297 packages in May2019)\nBioconductor (1741 packages in May 2019) (bioscience data analysis)\n\nThis huge variety of packages is one of the reasons why R is so successful: the chances are that someone has already solved a problem that you‚Äôre working on, and you can benefit from their work by downloading their package for free.\n\n\n\nInstalling packages\nIn this tutorial you will not use any packages. However, if you continue to use R for biodata analysis you will surely need to install many useful packages, both from CRAN and from Bioconductor (R repositories), and from other code repositories such as GitHub.\n\nHow to Install and Load packages\nThere are several alternative ways to install packages in R. Depending on the repository from which you want to install a package, there are dedicated functions that facilitate this task:\n\ninstall.packages() built-in function to install packages from the CRAN repository;\nBiocManager::install() to install packages from the Bioconductor repository;\nremotes::install_github to install packages from GitHub (a code repository, not exclusively dedicated to R).\n\nAfter installing a package, you must load it to make its contents (functions and/or data) available. The loading is done with the function library(). Alternatively, you can prepend the name of the package followed by :: to the function name to use it (e.g.¬†ggplot2::qplot()).\nFirst lets learn how to install packages from CRAN and from Bioconductor. To install packages from CRAN, R provides the install.packages() function, and any installed package, to be used, must be loaded via the library() function.\n# install the package called ggplot2\ninstall.packages (\"ggplot2\")   \n\n# load the library ggplot2\nlibrary (\"ggplot2\")     \nTo install packages from Bioconductor, you must first install Bioconductor:\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install()\nOnce installed, you can now download packages from Bioconductor using the BiocManager::install() function. The package AnnotationDbi is one of the most used in the context of genomics. Lets install it as an example of how to install packages from the Bioconductor repository:\n# Install the package\nBiocManager::install(\"AnnotationDbi\")\n\n# Load the package\nlibrary(\"AnnotationDbi\")\nNow the functions provided by the ggplot2 and the AnnotationDbi packages are available to be used in R.\nTo install packages from GitHub, the easiest way is to install the remotes package first, and then install the package of interest.\n# Make sure you have the {remotes} installed:\ninstall.packages('remotes')\n\n# Now you can install the ualg.compbio package from GitHub with:\nremotes::install_github(\"instructr/ualg.compbio\")\nNow you will have the tutorials provided by the ualg.compbio package.\n\n\n\nGetting help\nBut how to get information/help on how to use any function in R? There are many built-in ways in which R can provide help regarding its functions and packages:\n# help(package=\"package_name\") to get help about a specific package\nhelp (package=ggplot2) \n\n# show a pdf with the package manual (called R vignettes)\nvignette (\"ggplot2\")   \n\n# ?function to get quick info about the function of interest\n?qplot  \n\n\n\nWorking environment\nYour working environment is the place where the variables, functions, and data that you create are stored. More advanced users can create more than one environment.\nls()    # list all objects in your environment\ndir()   # list all files in your working directory\ngetwd() # find out the path to your working directory\nsetwd(\"/home/foo/bar/DATA/\") # example of setting a new working directory path\n\n\n\n\nSelf-paced tutorial\n\n\nCreate an RStudio project (30 min)\nTo start we will open RStudio. This is an Integrated Development Environment - IDE - that includes syntax-highlighting text editor (1 in Figure1), an R console to execute code (2 in Figure1), as well as workspace and history management (3 in Figure1), and tools for plotting and exporting images, browsing the workspace, managing packages and viewing html/pdf files created within RStudio (4 in Figure1).\n\n\n\nFigure 1: RStudio Graphical User Interface (GUI)\n\n\nProjects are a great functionality, easing the transition between different dataset analyses, and allowing a fast navigation to your analysis/working directory. To create a new project:\nFile &gt; New Project... &gt; New Directory &gt; New Project\nDirectory name: r-absoluteBeginners\nCreate project as a subdirectory of: ~/\n                           Browse... (directory/folder to save the workshop data)\nCreate Project\nProjects should be personalized by clicking on the menu in the right upper corner. The general options - R General - are the most important to customize, since they allow the definition of the RStudio ‚Äúbehavior‚Äù when the project is opened. The following suggestions are particularly useful:\nRestore .RData at startup - Yes (for analyses with +1GB of data, you should choose \"No\")\nSave .RData on exit - Ask\nAlways save history - Yes\n\n\n\nFigure 2: Customize Project\n\n\n\n\n\nOperators (60 min)\nImportant NOTE: Please create a new R Script file to save all the code you use for today‚Äôs tutorial and save it in your current working directory. Name it: r4ab_day1.R\n\n\nAssignment operators\nValues are assigned to named variables with an &lt;- (arrow) or an = (equal) sign. In most cases they are interchangeable, however it is good practice to use the arrow since it is explicit about the direction of the assignment. If the equal sign is used, the assignment occurs from left to right.\nx &lt;- 7     # assign the number 7 to a variable named x\nx          # R will print the value associated with variable x\n\ny &lt;- 9     # assign the number 9 to the variable y\n\nz = 3      # assign the value 3 to the variable z\n\n42 -&gt; lue  # assign the value 42 to the variable named lue\n\nx -&gt;  xx   # assign the value of x (which is the number 7) to the variable named xx\nxx         # print the value of xx\n\nmy_variable = 5   # assign the number 5 to the variable named my_variable\n\n\n\nComparison operators\nAllow the direct comparison between values, and its result is always a TRUE or FALSE value:\n\n\n\nSymbol\nDescription\n\n\n\n\n==\nexactly the same (equal)\n\n\n!=\ndifferent (not equal)\n\n\n&lt;\nsmaller than\n\n\n&gt;\ngreater than\n\n\n&lt;=\nsmaller or equal\n\n\n&gt;=\ngreater or equal\n\n\n\n1 == 1   # TRUE\n1 != 1   # FALSE\nx &gt; 3    # TRUE (x is 7)\ny &lt;= 9   # TRUE (y is 9)\nmy_variable &lt; z   # FALSE (z is 3 and my_variable is 5)\n\n\n\nLogical operators\nCompare logical (TRUE or FALSE) values:\n\n\n\nSymbol\nDescription\n\n\n\n\n&\nAND (vectorized)\n\n\n&&\nAND (non-vectorized/evaluates only the first value)\n\n\n|\nOR (vectorized)\n\n\n||\nOR (non-vectorized/evaluates only the first value)\n\n\n!\nNOT\n\n\n\nQUESTION: Are these TRUE, or FALSE?\nx &lt; y & x &gt; 10   # AND means that both expressions have to be true to return TRUE\nx &lt; y | x &gt; 10   # OR means that only one expression must be true to return TRUE\n!(x != y & my_variable &lt;= y)  # yet another AND example using NOT\n\n\n\nArithmetic operators\nR makes calculations using the following arithmetic operators:\n\n\n\nSymbol\nDescription\n\n\n\n\n+\nsummation\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^\npower\n\n\n\n3 / y   ## 0.3333333\nx * 2   ## 14\n3 - 4   ## -1\n2^z     ## 8\nmy_variable + 2   ## 7\n\n\n\n\n\nData structures (120 min)\nR has 5 basic data structures (see following figure).\n\n\n\nFigure 3: Basic R data structures.\n\n\n\nVectors\nThe basic data structure in R is the vector, which requires all of its elements to be of the same type (e.g.¬†all numeric; all character (text); all logical (TRUE or FALSE)).\n\n\n\nCreating vectors\n\n\n\nFunction\nDescription\n\n\n\n\nc\ncombine\n\n\n:\ninteger sequence\n\n\nseq\ngeneral sequence\n\n\nrep\nrepetitive patterns\n\n\n\nx &lt;- c (1,2,3,4,5,6)\nx\nclass (x)   # this function outputs the class of the object\n\ny &lt;- 10\nclass (y)\n\nz &lt;- \"a string\"\nclass (z)\n# The results are shown in the comments next to each line\n\nseq (1,6)   ## 1 2 3 4 5 6\nseq (from=100, by=1, length=5)   ## 100 101 102 103 104\n\n1:6    ## 1 2 3 4 5 6\n10:1   ## 10  9  8  7  6  5  4  3  2  1\n\nrep (1:2, 3)   ## 1 2 1 2 1 2\n\n\n\nVectorized arithmetics\nMost arithmetic operations in the R language are vectorized, i.e.¬†the operation is applied element-wise. When one operand is shorter than the other, the shortest one is recycled, i.e.¬†the values from the shorter vector are re-used until the length of the longer vector is reached.\nPlease note that when one of the vectors is recycled, a warning is printed in the R Console. This warning is not an error, i.e.¬†the operation has been completed despite the warning message.\n1:3 + 10:12\n\n# Notice the warning: this is recycling (the shorter vector \"restarts\" the \"cycling\")\n1:5 + 10:12\n\nx + y         # Remember that x = c(1,2,3,4,5,6) and y = 10\nc(70,80) + x\n\n\n\nSubsetting/Indexing vectors\nSubsetting is one of the most powerful features of R. It is the extraction of one or more elements, which are of interest, from vectors, allowing for example the filtering of data, the re-ordering of tables, removal of unwanted data-points, etc. There are several ways of sub-setting data.\nNote: Please remember that indices in R are 1-based (see introduction).\n# Subsetting by indices\nmyVec &lt;- 1:26 ; myVec\nmyVec [1]    # prints the first value of myVec\nmyVec [6:9]  # prints the 6th, 7th, 8th, and 9th values of myVec \n\n# LETTERS is a built-in vector with the 26 letters of the alphabet\nmyLOL &lt;- LETTERS       # assign the 26 letters to the vector named myLOL\nmyLOL[c(3,3,13,1,18)]  # print the requested positions of vector myLOL\n\n#Subsetting by same length logical vectors\nmyLogical &lt;- myVec &gt; 10 ; myLogical\n# returns only the values in positions corresponding to TRUE in the logical vector\nmyVec [myLogical]\n\n\n\nNaming indexes of a vector\nReferring to an index by name rather than by position can make code more readable and flexible. Use the function names to attribute names to each position of the vector.\njoe &lt;- c (24, 1.70)\nnames (joe)              ## NULL\nnames (joe) &lt;- c (\"age\",\"height\")\nnames (joe)              ## \"age\"    \"height\"\njoe [\"age\"] == joe [1]   ## age   TRUE\n\nnames (myVec) &lt;- LETTERS\nmyVec\n# Subsetting by field names\nmyVec [c(\"A\", \"A\", \"B\", \"C\", \"E\", \"H\", \"M\")] ## The Fibonacci Series :o)\n\n\n\nExcluding elements\nSometimes we want to retain most elements of a vector, except for one or a few unwanted positions. Instead of specifying all elements of interest, it is easier to specify the ones we want to remove. This is easily done using the minus sign.\nalphabet &lt;- LETTERS\nalphabet   # print vector alphabet\nvowel.positions &lt;- c(1,5,9,15,21)\nalphabet[vowel.positions]    # print alphabet in vowel.positions\n\nconsonants &lt;- alphabet [-vowel.positions]  # exclude all vowels from the alphabet\nconsonants\n\n\n\nMatrices\nMatrices are two dimensional vectors (tables), where all columns are of the same length, and, just like one-dimensional vectors, matrices store same-type elements (e.g.¬†all numeric; all character (text); all logical (TRUE or FALSE)). Matrices are explicitly created with the matrix function.\nIMPORTANT NOTE: R uses a column-major order for the internal linear storage of array values, meaning that first all of column 1 is stored, then all of column 2, etc. This implies that, by default, when you create a matrix, R will populate the first column, then the second, then the third, and so on until all values given to the matrix function are used. This is the default behavior of the matrix function, which can be changed via the byrow parameter (default value is set to FALSE).\nmy.matrix &lt;- matrix (1:12, nrow=3, byrow = FALSE)   # byrow = FALSE is the default (see ?matrix) \ndim (my.matrix)   # check the dimension (size) of the matrix: number of rows (first number) and number of columns (second number)\nmy.matrix         # print the matrix\n\nxx &lt;- matrix (1:12, nrow=3, byrow = TRUE)\ndim (xx)  # check if the dimensions of xx are the same as the dimensions of my.matrix\nxx        # compare my.matrix with xx and make sure you understand what is hapenning\n\n\n\nSubsetting/Indexing matrices\nVery Important Note: The arguments inside the square brackets in matrices (and data.frames - see next section) are the [row_number, column_number]. If any of these is omitted, R assumes that all values are to be used: all rows, if the first value before the comma is missing; or all columns if the second value after the comma is missing.\n# Creating a matrix of characters\nmy.matrix &lt;- matrix (LETTERS, nrow = 4, byrow = TRUE) \n# Please notice the warning message (related to the \"recycling\" of the LETTERS)\n\nmy.matrix         # print the matrix\ndim (my.matrix)   # check the dimensions of the matrix\n\n# Subsetting by indices \nmy.matrix [,2]   # all rows, column 2 (returns a vector)\nmy.matrix [3,]   # row 3, all columns (returns a vector)\nmy.matrix [1:3,c(4,2)]   # rows 1, 2 and 3 from columns 4 and 2 (by this order) (returns a matrix)\n\n\n\nData frames\nData frames are the most flexible and commonly used R data structures, used to store datasets in spreadsheet-like tables.\nIn a data.frame, usually the observations are the rows and the variables are the columns. Unlike matrices, the columns of a data frame can be vectors of different types (i.e.¬†text, number, logical, etc, can all be stored in the same data frame). However, each column must to be of the same data type.\ndf &lt;- data.frame (type=rep(c(\"case\",\"control\"),c(2,3)),time=rnorm(5))  \n# rnorm is a random number generator retrieved from a normal distribution\n\nclass (df)   ## \"data.frame\"\ndf\n\n\n\nSubsetting/Indexing Data frames\nData frames are easily subset by index number using the square brackets notation [], or by column name using the dollar sign $.\nRemember: The arguments inside the square brackets, just like in matrices, are the [row_number, column_number]. If any of these is omitted, R assumes that all values are to be used.\nNOTE: R includes a package in its default base installation, named ‚ÄúThe R Datasets Package‚Äù. This resource includes a diverse group of datasets, containing data from different fields: biology, physics, chemistry, economics, psychology, mathematics. These data are very useful to learn R. For more info about these datasets, run the following command: library(help=datasets)\nHere we will use the classic iris dataset to explore data frames, and learn how to subset them.\n# Familiarize yourself with the iris dataset (built-in dataset with measurements of iris flowers)\niris\n\n# Subset by indices the iris dataset\niris [,3]   # all rows, column 3 \niris [1,]   # row 1, all columns\niris [1:9, c(3,4,1,2)]   # rows 1 to 9 with columns 3, 4, 1 and 2 (in this order)\n\n# Subset by column name (for data.frames)\niris$Species            #show only the species column\niris[,\"Sepal.Length\"]\n\n# Select the time column from the df data frame created above\ndf$time      ## 0.5229577 0.7732990 2.1108504 0.4792064 1.3923535\n\n\n\nLists\nLists are very powerful data structures, consisting of ordered sets of elements, that can be arbitrary R objects (vectors, strings, functions, etc), and heterogeneous, i.e.¬†each element of a different type.\nlst = list (a=1:3, b=\"hello\", fn=sqrt)   # index 3 contains the function \"square root\"\nlst\nlst$fn(49)   # outputs the square root of 49\n\n\n\nSubsetting/Indexing lists\nLike data frames they can be subset both by index number (inside square brackets) or by name using the dollar sign.\nNOTE: There is one subsetting feature that is particular to lists, which is the possibility of indexing using single square brackets [ ], or double square-brackets [[ ]]. The difference between these are the fact that, single brackets always return a list, while double brackets return the object in its native type (the same occurs with the dollar sign). For example, if the 3rd element of my.list is a data frame, then indexing the list using my.list[3] will return a list, of size 1 storing a data frame; but indexing it using my.list[[3]] will return the data frame itself.\n# Subsetting by indices\nlst [1]     # returns a list with the data contained in position 1 (preserves the type of data as list)\nclass (lst[1])\n\nlst [[1]]   # returns the data contained in position 1 (simplifies to inner data type) \nclass(lst[[1]])\n\n# Subsetting by name\nlst$b       # returns the data contained in position 1 (simplifies to inner data type)\nclass(lst$b)\n\n# Compare the class of these alternative indexing by name\nlst[\"a\"]\nlst[[\"a\"]]\n\n\n\nFactors\nFactors are variables in R which take on a limited number of different values - such variables are often refered to as categorical variables.\n\n\n\nFigure 4: Main types of variables.\n\n\n\n‚ÄúOne of the most important uses of factors is in statistical modeling; since categorical variables enter into statistical models differently than continuous variables, storing data as factors insures that the modeling functions will treat such data correctly.\nFactors in R are stored as a vector of integer values with a corresponding set of character values to use when the factor is displayed. The factor function is used to create a factor. The only required argument to factor is a vector of values which will be returned as a vector of factor values. Both numeric and character variables can be made into factors, but a factor‚Äôs levels will always be character values. You can see the possible levels for a factor through the levels command.\nFactors represent a very efficient way to store character values, because each unique character value is stored only once, and the data itself is stored as a vector of integers.‚Äù Because of this, read.table used to convert character variables to factors by default. Since version X of R this has changed, and now the argument stringsAsFactors = FALSE is the default.\n(Adapted from: https://www.stat.berkeley.edu/~s133/factors.html)\n# Create a vector of numbers to be displayed as Roman Numerals\nmy.fdata &lt;- c(1,2,2,3,1,2,3,3,1,2,3,3,1)\n# look at the vector\nmy.fdata\n\n# turn the data into factors\nfactor.data &lt;- factor(my.fdata)\n# look at the factors\nfactor.data\n\n# add labels to the levels of the data\nlabeled.data &lt;- factor(my.fdata,labels=c(\"I\",\"II\",\"III\"))\n# look at the factors\nlabeled.data\n# look only at the levels (i.e. character labels) of the factors\nlevels(labeled.data)\n\n\nData structure conversion\nData structures can be inter-converted (coerced) from one type to another. Sometimes it is useful to convert between data structure types (particularly when using packages).\nNOTE: Such conversions are not always possible without information loss - for example converting a data frame with mix data types to a matrix is not possible without converting all columns to the same type, possibly leading to losses.\nR has several functions for data structure conversions:\n# To check the class of the object:\nclass(lst)\n\n# To check the basic structure of an object:\nstr(lst)\n\n# \"Force\" the object to be of a certain type:\n # (this is not valid code, just a syntax example)\nas.matrix (myDataFrame)  # convert a data frame into a matrix\nas.numeric (myChar)      # convert text characters into numbers\nas.data.frame (myMatrix) # convert a matrix into a data frame\nas.character (myNumeric) # convert numbers into text chars\n\n\n\n\nLoops and Conditionals in R (60 min)\n\nfor () and while () loops\nR allows the implementation of loops, i.e.¬†replicating instructions in an iterative way (also called cycles). The most common ones are for () loops and while () loops. The syntax for these loops is: for (condition) { code-block } and while (condition) { code-block }.\n# creating a for loop to calculate the first 12 values of the Fibonacci sequence\nmy.x &lt;- c(1,1)\nfor (i in 1:10) {\n  my.x &lt;- c(my.x, my.x[i] + my.x[i+1])\n  print(my.x)\n}\n\n# while loops will execute a block of commands until a condition is no longer satisfied\nx &lt;- 3 ; x\nwhile (x &lt; 9)\n{\n  cat(\"Number\", x, \"is smaller than 9.\\n\") # cat is a printing function (see ?cat)\n   x &lt;- x+1\n}\n\n\n\nConditionals: if () statements\nConditionals allow running commands only when certain conditions are TRUE. The syntax is: if (condition) { code-block }.\nx &lt;- -5 ; x\nif (x &gt;= 0) { print(\"Non-negative number\") } else { print(\"Negative number\") }\n # Note: The else clause is optional. If the command is run at the command-line,\n  # and there is an else clause, then either all the expressions must be enclosed\n  # in curly braces, or the else statement must be in line with the if clause.\n\n# coupled with a for loop\nx &lt;- c(-5:5) ; x\nfor (i in 1:length(x)) {\n  if (x[i] &gt; 0) {\n     print(x[i])\n  } \n  else {\n   print (\"negative number\")\n  }\n}  \n\n\n\nConditionals: ifelse () statements\nThe ifelse function combines element-wise operations (vectorized) and filtering with a condition that is evaluated. The major advantage of the ifelse over the standard if-then-else statement is that it is vectorized. The syntax is: ifelse (condition-to-test, value-for-true, value-for-false).\n# re-code gender 1 as F (female) and 2 as M (male)\ngender &lt;- c(1,1,1,2,2,1,2,1,2,1,1,1,2,2,2,2,2)\nifelse(gender == 1, \"F\", \"M\")\n\n\n\n\nFunctions (60 min)\nR allows defining new functions using the function command. The syntax (in pseudo-code) is the following:\nmy.function.name &lt;- function (argument1, argument2, ...) { \n  expression1\n  expression2\n  ...\n  return (value)\n  }\nNow, lets code our own function to calculate the average (or mean) of the values from a vector:\n# Define the function\n    # Please note that the function must be declared in the script before it can be used\nmy.average &lt;- function (x) {\n  average.result &lt;- sum(x)/length(x)\n  return (average.result)\n}\n\n# Create the data vector\nmy.data &lt;- c(10,20,30)\n\n# Run the function using the vector as argument\nmy.average(my.data)\n\n# Compare with R built-in mean function\nmean(my.data)\n\n\n\nLoading data and Saving files (30 min)\nMost R users need to load their own datasets, usually saved as table files (e.g.¬†Excel, or .csv files), to be able to analyse and manipulate them. After the analysis, the results need to be exported/saved (e.g.¬†to view or use with other software).\n# Inspect the esoph built-in dataset\nesoph\ndim(esoph)\ncolnames(esoph)\n\n### Saving ###\n# Save to a file named esophData.csv the esoph R dataset, separated by commas and\n # without quotes (the file will be saved in the current working directory)\nwrite.table (esoph, file=\"esophData.csv\", sep=\",\" , quote=F)\n\n# Save to a file named esophData.tab the esoph dataset, separated by tabs and without\n # quotes (the file will be saved in the current working directory)\nwrite.table (esoph, file=\"esophData.tab\", sep=\"\\t\" , quote=F)\n\n### Loading ###\n# Load a data file into R (the file should be in the working directory)\n  # read a table with columns separated by tabs\nmy.data.tab &lt;- read.table (\"esophData.tab\", sep=\"\\t\", header=TRUE)\n # read a table with columns separated by commas\nmy.data.csv &lt;- read.csv (\"esophData.csv\", header=T)\nNote: if you want to load or save the files in directories different from the working directory, just use (inside quotes) the full path as the first argument, instead of just the file name (e.g.¬†‚Äú/home/Desktop/r_Workshop/esophData.csv‚Äù).\n\n\n\nSome great R functions to ‚Äúplay‚Äù with (60 min)\n\nUsing the iris buil-in dataset\n# the unique function returns a vector with unique entries only (remove duplicated elements)\nunique (iris$Sepal.Length)\n\n# length returns the size of the vector (i.e. the number of elements)\nlength (unique (iris$Sepal.Length))\n\n# table counts the occurrences of entries (tally)\ntable (iris$Species)\n\n# aggregate computes statistics of data aggregates (groups)\naggregate (iris[,1:4], by=list (iris$Species), FUN=mean, na.rm=T)\n\n# the %in% function returns the intersection between two vectors\nmonth.name [month.name %in% c(\"CCMar\",\"May\", \"Fish\", \"July\", \"September\",\"Cool\")]\n\n# merge joins data frames based on a common column (that functions as a \"key\")\ndf1 &lt;- data.frame(x=1:5, y=LETTERS[1:5]) ; df1\ndf2 &lt;- data.frame(x=c(\"Eu\",\"Tu\",\"Ele\"), y=1:6) ; df2\nmerge (df1, df2, by.x=1, by.y=2, all = TRUE)\n\n# cbind and rbind (takes a sequence of vector, matrix or data-frame arguments\n # and combine them by columns or rows, respectively)\nmy.binding &lt;- as.data.frame(cbind(1:7, LETTERS[1:7]))    # the '1' (shorter vector) is recycled\nmy.binding\nmy.binding &lt;- cbind(my.binding, 8:14)[, c(1, 3, 2)] # insert a new column and re-order them\nmy.binding\n\nmy.binding2 &lt;- rbind(seq(1,21,by=2), c(1:11))\nmy.binding2\n\n# reverse the vector\nrev (LETTERS)\n# sum and cumulative sum\nsum (1:50); cumsum (1:50)\n# product and cumulative product\nprod (1:25); cumprod (1:25)\n\n### Playing with some R built-in datasets (see library(help=datasets) )\niris   # familiarize yourself with the iris data\n\n# mean, standard deviation, variance and median \nmean (iris[,2]); sd (iris[,2]); var (iris[,2]); median (iris[,2]) \n\n# minimum, maximum, range and summary statistics\nmin (iris[,1]); max (iris[,1]); range (iris[,1]); summary (iris)\n\n# exponential, logarithm\nexp (iris[1,1:4]); log (iris[1,1:4])\n\n# sine, cosine and tangent (radians, not degrees)\nsin (iris[1,1:4]); cos (iris[1,1:4]); tan (iris[1,1:4]) \n\n# sort, order and rank the vector\nsort (iris[1,1:4]); order (iris[1,1:4]); rank (iris[1,1:4])\n\n# useful to be used with if conditionals\nany (iris[1,1:4] &gt; 2)   # ask R if there are any values higher that 2? \nall (iris[1,1:4] &gt; 2)   # ask R if all values are higher than 2\n\n# select data\nwhich (iris[1,1:4] &gt; 2)\nwhich.max (iris[1,1:4]) \n\n# subset data by values/patterns from different columns \nsubset(iris, Petal.Length &gt;= 3 & Sepal.Length &gt;= 6.5, select=c(Petal.Length, Sepal.Length, Species))\n\n\n\nUsing the esoph buil-in dataset\nThe esoph (Smoking, Alcohol and (O)esophageal Cancer data) built-in dataset presents 2 types of variables: continuous numerical variables (the number of cases and the number of controls), and discrete categorical variables (the age group, the tobacco smoking group and the alcohol drinking group). Sometimes it is hard to ‚Äúcategorize‚Äù continuous variables, i.e.¬†to group them in specific intervals of interest, and name these groups (also called levels).\nAccordingly, imagine that we are interested in classifying the number of cancer cases according to their occurrence: frequent, intermediate and rare. This type of variable re-coding into factors is easily accomplished using the function cut(), which divides the range of x into intervals and codes the values in x according to which interval they fall.\n# subset non-contiguous data from the esoph dataset\nesoph\nsummary(esoph)\n# cancers in patients consuming more than 30 g/day of tobacco\nsubset(esoph$ncases, esoph$tobgp == \"30+\")\n# total nr of cancers in patients older than 75\nsum(subset(esoph$ncases, esoph$agegp == \"75+\"))\n\n# factorize the nr of cases in 3 levels, equally spaced,\n # and add the new column named cat_ncases, to the dataset\nesoph$cat_ncases &lt;- cut (esoph$ncases,3,labels=c(\"rare\",\"med\",\"freq\"))\nsummary(esoph)\nThe end"
  },
  {
    "objectID": "week2.html#tidy-data-structuring-data-for-analysis",
    "href": "week2.html#tidy-data-structuring-data-for-analysis",
    "title": "Week 2 | 2. R Intro & Tidy data",
    "section": "Tidy Data: Structuring Data for Analysis",
    "text": "Tidy Data: Structuring Data for Analysis\n\nData Tidying\nA significant portion of data analysis involves cleaning and preparing data. This process is iterative, as new issues arise or additional data is collected. A key aspect of cleaning is data tidying, which focuses on structuring datasets to facilitate analysis.\nThe principles of tidy data provide a standardized way to organize data, simplifying initial cleaning and ensuring compatibility with analysis tools. Tidy datasets reduce the need for data transformation between tools, allowing analysts to focus on meaningful insights rather than data logistics.\n\n\nDefining Tidy Data\nTidy data aligns the structure of a dataset with its meaning. It follows three key principles:\n\nEach variable is a column.\nEach observation is a row.\nEach value is a single cell.\n\n\nThis structure makes analysis more efficient by ensuring that variables and observations are consistently represented.\n\n\nData Structure & Semantics\nData consists of values that belong to:\n\nVariables: Attributes measured across units (e.g., height, score).\nObservations: Measurements for a single unit (e.g., a person, a test result).\n\nDifferent structures can represent the same data. For instance, wide and long formats can hold identical information but differ in layout. A tidy format ensures that each row represents a distinct observation, making relationships between values explicit.\n\n\n\nBenefits of Tidy Data\n\nEasier manipulation: Standardized structure allows simple extraction and transformation.\nError reduction: A consistent format prevents mistakes in processing.\nBetter tool compatibility: Tidy datasets integrate seamlessly with vectorized programming languages like R.\nEfficient analysis: Aggregations and comparisons are straightforward.\n\n\n\n\nOrganizing Variables & Observations\n\nVariables can be fixed (defined by study design) or measured (collected during the study).\nOrdering variables logically improves readability and comprehension.\nData should be arranged so that related observations and variables are contiguous, improving interpretability.\n\n\n\nTake home message\nBy following tidy data principles, researchers can streamline workflows, minimize errors, and enhance the clarity of their datasets.\n\n\n\nCitations\n\nIllustrations from the Openscapes blog: Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst (https://openscapes.org/blog/2020-10-12-tidy-data/)\nContent based on:\n\n\nhttps://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1‚Äì23. https://doi.org/10.18637/jss.v059.i10"
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "Week 5 | 3.1 Exploratory analysis critique",
    "section": "",
    "text": "Class Details\n\n\n\nüìÖ Date: 08 April, 2025\n‚è∞ Time: 09:30h - 12:00h\nüìñ Synopsis: Metadata documentation, data visualization for validation, critical review of scientific datasets and their publications.\n\n\n\nDone in ClassTo Do at Home\n\n\n\n1. Metadata Files\n\nDiscussion Topics:\n\nWhat is a metadata file, and why is it important for reproducibility?\nMinimum information to include:\n\nVariable names and descriptions\nUnits of measurement\nData types (e.g., numeric, categorical)\nValid value ranges\nMissing value encoding\n\nEncoding not applicable or missing values:\n\nUse standard codes (e.g., NA, null, or defined codes like 999), and document them clearly\n\nCategorical variables:\n\nDefine all possible levels\nInclude labels or descriptions for each level, if acronym is being used\n\nBest practices:\n\nKeep metadata in a separate, human-readable file (e.g., CSV, TXT)\nUse consistent naming between metadata and data files\nRegularly update metadata as data evolves\n\n\nActivity:\n\nAnalyze and discuss a good example of data and metadata from a Figshare dataset record (Link here).\nLook at the paper that analysis the dataset above: (Link here)\n\n\n\n\n\n2. Principal Component Analysis (PCA)\n\nDiscussion Topics:\n\nCan categorical variables be included in PCA?\n\nGenerally, PCA is for continuous variables\nCategorical variables need to be encoded numerically (e.g., one-hot encoding), which may affect interpretation\nConsider alternative techniques like Multiple Correspondence Analysis (MCA) for purely categorical data\n\n\nResource:\n\nWatch and evaluate StatQuest video on PCA (Link here)\n\n\n\n\n\n3. Visualizing Distributions of Variables\n\nWhat to look for in plots:\n\nOutliers\nSkewness or symmetry\nBimodal or multimodal distributions\nUnexpected patterns or values\n\nQuestions to ask when selecting plots:\n\nWhat story does this plot tell? What information does it convey? Is this information what I am aiming to show?\nIs the plot relevant to the hypothesis or goal of my analysis?\nIs it easy to interpret by the target audience?\nAre there simpler visualizations for this variable? ‚ÄúLess is more!‚Äù\nUse previous results/plots/analyses to inform which plots to show next, and keep in the main text versus sending to supplementary material.\n\nData validation through visualization:\n\nIdentify data entry errors or inconsistencies\nSpot missing or unusual values\nConfirm assumptions (e.g., normality, bimodality‚Ä¶)\nHelp ensure robusteness and reliability\nWhenever possible, show all data points (Transparency is key for reproducibility)\n\nChoose your plots wisely!\nUseful online galleries of plots, per type of data or information to be displayed:\n\nIncludes R code for all plots: from Data to Viz\nVisual vocabulary\nThe Do‚Äôs of chart design\nFinantial Times: Chart-Doctor\n\n\n\n\n\n4. Critical Review of Nature Scientific Data Papers\n\nPapers for student-led discussion:\n\nPaper 1: High-quality genome assembly of the azooxanthellate coral Tubastraea coccinea (Lesson, 1829)\nDOI: https://doi.org/10.1038/s41597-025-04839-7\n\nPaper 2: Seagrass morphometrics at species level in Moreton Bay, Australia from 2012 to 2013\nDOI: https://doi.org/10.1038/sdata.2017.60\n\nGuiding questions for critical review of a data descriptor article:\n\nDid the authors successfully show the relevance of their data to their scientific community?\nIs the data of high quality?\nAre all visualizations necessary and informative?\n\nAre they easy to interpret?\nAre they well-described?\n\nIs any crucial information, about the data or the experimental design, missing?\nWould you feel confident using this dataset in a new analysis? Why? Why not?\n\n\n\n\n\n\nExperimental Design figure, in vectorial format (pdf, eps, svg).\n\nTop Tips for an Experimental Design Figure\n\nKeep it simple and clear\n\nShow the workflow or timeline of the experiment\n\nInclude key elements: groups, treatments, time points, sample sizes\n\nUse consistent labels and colors\n\nMake sure it‚Äôs readable and matches the methods\n\nExport as a vector (PDF/SVG) for best quality\n\n\nNext Week, each student will receive individual R code review. Please complete and format your R code, to the best of your current ability. Focus areas for next week‚Äôs feedback will be:\n\nCode correctness and clarity\nAnnotation and commenting practices\nProper use of literate programming (e.g., R Markdown)\nAdherence to best practices in reproducible research workflows (Git)\n\n\nPlease make notes of any difficulties and questions to guide our discussion next week.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "Week 1 | 1. System setup",
    "section": "",
    "text": "Class Details\n\n\n\nüìÖ Date: 05 March, 2025\n‚è∞ Time: 10:30h ‚Äì 12:30h\nüìñ Synopsis: Setting up the computer system for reproducible data analysis\n\n\n\n\nPrerequisites | Mandatory\n\nYou should bring your own laptop. This course teaches essential data analysis skills, including setting up and managing your computing environment. Bringing your own laptop ensures you can apply what you learn, practice continuously, and integrate it into your research.\n\nIf you do not have one, an RStudio Server account can be created.\n\nYou must have administrator privileges to install software in your computer;\n\n\n\nHow to setup your system for reproducible data analysis\nReproducibility is a cornerstone of reliable data analysis, ensuring that results can be consistently replicated and built upon.\nSetting up a system for reproducible data analysis requires careful planning and adherence to best practices. Bellow is an overview of the major steps a researcher should take into consideration.\n\n\n\nTools used for this course\nIn this course, we will use R and RStudio for data analysis, and Git and GitHub for version control:\n\nR is a powerful programming language designed for statistical computing and data analysis.\n\nRStudio is an integrated development environment (IDE) that provides a user-friendly interface for writing, debugging, and managing R code.\n\nGit is a version control system that tracks changes in code, allowing for efficient collaboration and history management.\n\nGitHub is a cloud-based platform for hosting Git repositories, enabling code sharing, collaboration, and version tracking across teams.\n\nBy combining these tools, researchers can streamline their workflows, ensure reproducibility, and foster collaboration and efficiency in data-driven projects.\n\n\nInstall software\n\nFirst install R: https://cran.r-project.org/\nThen install RStudio: https://posit.co/download/rstudio-desktop/\nInstall git: https://git-scm.com/downloads\n- Detailed instructions for each Operating System (Linux, Mac, Windows) here.\nCreate an account in GitHub: https://docs.github.com/en/get-started/start-your-journey/creating-an-account-on-github\n\n\n\nNext steps\n\n\n\n\n Back to top"
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "R Training materials",
    "section": "",
    "text": "The training materials that I have developed for past R courses are open and freely available online:\n\nR for Absolute Beginners (2022)\nTidy Data & Visualization with R (2022)\nPublication-Ready Graphs using R (2023)\nTranscriptomics data analysis (2023)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "Week 3 | 2.1 Identify variables & Tidy data",
    "section": "",
    "text": "Class Details\n\n\n\nüìÖ Date: 20 March, 2025\n‚è∞ Time: 09:30h - 11:30h\nüìñ Synopsis: Extract experimental details from text, format tidy tables, and reverse engineer study design from tidy tables.\n\n\n\n\nTheoryExercise 1Exercise 2Exercise 3Food for thought\n\n\n\nExperimental Designs\n\n1. Controlled Experiment\n\nDescription: Researcher manipulates one or more variables while keeping others constant.\nExample: Adjusting pH levels in aquaria and measuring coral growth.\nStrengths: Clear cause-effect relationships, high internal validity.\nLimitations: May lack realism in natural environments.\n\n\n\n2. Observational Study\n\nDescription: No manipulation; variables are observed as they naturally occur.\nExample: Comparing fish diversity in protected vs.¬†unprotected areas.\nTypes:\n\nCross-sectional: Single point in time.\nLongitudinal (Time-series): Data collected over time.\n\nStrengths: Realistic, practical.\nLimitations: Cannot infer causation definitively.\n\n\n\n3. Correlational Study\n\nDescription: Measures the relationship between variables without manipulation.\nExample: Monitoring algal biomass and oxygen levels to detect patterns.\nStrengths: Good for detecting associations.\nLimitations: No causal conclusions.\n\n\n\n4. Randomized Controlled Trial (RCT)\n\nDescription: Random assignment of subjects/units to treatments and control.\nExample: Randomly assigning noise levels to dolphin enclosures.\nStrengths: Minimizes bias, strong causal inference.\nLimitations: Logistically complex in the field.\n\n\n\n5. Factorial Design\n\nDescription: Tests multiple independent variables simultaneously.\nExample: Testing combined effects of temperature and salinity on zooplankton.\nStrengths: Identifies interaction effects between variables.\nLimitations: Requires more samples.\n\n\n\n6. Before-After Control-Impact (BACI) Design\n\nDescription: Compares conditions before and after an impact at both affected and control sites.\nExample: Measuring water quality before and after coastal development.\nStrengths: Controls for natural variability.\nLimitations: Requires baseline data.\n\n\n\n7. Natural Experiment\n\nDescription: Utilizes naturally occurring events as treatments.\nExample: Studying fish populations after a hurricane.\nStrengths: High ecological validity.\nLimitations: Limited control over variables.\n\n\n\n\n\nGoal | This exercise aims at training students to critically extract key experimental details from textual descriptions.\nInstructions | Read each narrative description carefully. For each one, identify:\n\nExperimental Design (check the Experimental Designs tab)\nIndependent Variable(s)\nDependent Variable(s)\nControl Variable(s) (if applicable)\n\n\n\nExample 1: Ocean Acidification on Coral Growth\nResearchers want to test how increased ocean acidification affects the growth of a common coral species. They set up three aquaria with seawater at different pH levels: 8.1 (current ocean pH), 7.8, and 7.5. Identical coral fragments are placed in each aquarium, and all tanks are kept under the same light and temperature conditions. After six months, they measure the calcification rate of the corals.\n\n\n\n\n\n\nSolution 1 | Click to show\n\n\n\n\n\n\nExperimental Design: Controlled laboratory experiment with three treatment groups.\nIndependent Variable: pH level of seawater (8.1, 7.8, 7.5)\nDependent Variable: Coral calcification rate (growth)\nControl Variables: Light intensity, water temperature, coral species and fragment size, nutrient levels.\n\n\n\n\n\n\n\nExample 2: Marine Protected Areas and Fish Biodiversity\nA team surveys fish species diversity in 10 marine protected areas (MPAs) and compares it with 10 nearby unprotected coastal sites. Surveys are conducted using the same underwater visual census method at all locations during the same season.\n\n\n\n\n\n\nSolution 2 | Click to show\n\n\n\n\n\n\nExperimental Design: Observational comparative study (between protected and unprotected areas)\nIndependent Variable: Protection status (MPA vs.¬†unprotected site)\nDependent Variable: Fish species diversity (number of species observed)\nControl Variables: Survey method, season, location depth, time of day surveyed.\n\n\n\n\n\n\n\nExample 3: Plastic Pollution Impact on Zooplankton\nScientists investigate whether microplastic concentration affects zooplankton reproduction. They prepare four tanks with different microplastic concentrations: 0 particles/mL, 10 particles/mL, 100 particles/mL, and 1,000 particles/mL. Each tank contains the same number of zooplankton individuals and is kept at constant temperature, salinity, and food availability. After two weeks, they count the number of offspring produced.\n\n\n\n\n\n\nSolution 3 | Click to show\n\n\n\n\n\n\nExperimental Design: Controlled laboratory experiment with graded treatments.\nIndependent Variable: Microplastic concentration (0, 10, 100, 1,000 particles/mL)\nDependent Variable: Number of zooplankton offspring\nControl Variables: Temperature, salinity, food availability, initial zooplankton number/species.\n\n\n\n\n\n\n\nExample 4: Algal Bloom Effects on Oxygen Levels\nTo examine the effect of algal blooms on oxygen levels, researchers monitor dissolved oxygen in a coastal bay over six months. They record algal biomass, water temperature, and nutrient concentrations weekly. They look for correlations between algal biomass and oxygen concentration.\n\n\n\n\n\n\nSolution 4 | Click to show\n\n\n\n\n\n\nExperimental Design: Observational time-series study\nIndependent Variable: Algal biomass (measured continuously)\nDependent Variable: Dissolved oxygen concentration\nControl Variables: Recording temperature and nutrient levels allows adjustment for confounding variables.\n\n\n\n\n\n\n\nExample 5: Noise Pollution Impact on Dolphin Communication\nMarine biologists play varying levels of ship noise recordings in a controlled enclosure housing a pod of dolphins. For each noise level (low, medium, high), they observe and record the frequency and duration of dolphin whistles. Sessions are randomized, and the dolphins have rest periods between exposures.\n\n\n\n\n\n\nSolution 5 | Click to show\n\n\n\n\n\n\nExperimental Design: Randomized controlled experiment\nIndependent Variable: Ship noise level (low, medium, high)\nDependent Variables: Frequency and duration of dolphin whistles\nControl Variables: Same dolphin pod, rest periods, enclosure size, time of day.\n\n\n\n\n\n\n\nGoal | To bridge experimental design concepts with practical data skills.\nInstructions | After identifying the independent, dependent, and control variables from each ‚Äúnarrative‚Äù, describe how the data would be organized in a tidy data table. Specifically:\n\nColumns: What variables are measured or recorded?\nRows: What does each row represent (unit of observation)?\nVariable types: Are variables numeric or categorical?\nControl variables: How are they documented?\n\n\n\nExample 1: Ocean Acidification on Coral Growth\n\n\n\n\n\n\nSolution 1 | Click to show\n\n\n\n\n\nTidy Table Example:\n\n\n\n\n\n\n\n\n\n\n\n\ntank_id\npH\ncalcification_rate\nlight_intensity\ntemp_C\ncoral_species\nnutrient_conc\n\n\n\n\nTank_1\n8.1\n2.3\n200\n25\nAcropora sp.\n3\n\n\nTank_2\n7.8\n1.9\n200\n25\nAcropora sp.\n3\n\n\nTank_3\n7.5\n1.4\n200\n25\nAcropora sp.\n3\n\n\n\nEach row = one coral fragment after six months.\n\n\n\n\n\n\nExample 2: Marine Protected Areas and Fish Biodiversity\n\n\n\n\n\n\nSolution 2 | Click to show\n\n\n\n\n\nTidy Table Example:\n\n\n\n\n\n\n\n\n\n\n\n\nsite_id\nprotection_status\nfish_species_count\nsurvey_method\nseason\ndepth_m\ntime_of_day\n\n\n\n\nMPA_1\nMPA\n25\nVisual Census\nSpring\n10\nMorning\n\n\nMPA_2\nMPA\n30\nVisual Census\nSpring\n12\nMorning\n\n\nSite_1\nUnprotected\n18\nVisual Census\nSpring\n11\nMorning\n\n\nSite_2\nUnprotected\n20\nVisual Census\nSpring\n10\nMorning\n\n\n\nEach row = one survey at one site.\n\n\n\n\n\n\nExample 3: Plastic Pollution Impact on Zooplankton\n\n\n\n\n\n\nSolution 3 | Click to show\n\n\n\n\n\nTidy Table Example:\n\n\n\n\n\n\n\n\n\n\n\n\n\ntank_id\nmicroplastic_conc\noffspring_count\ntemp_C\nsalinity\nfood_available\nzooplankton_sp\ninitial_count\n\n\n\n\nTank_A\n0\n45\n22\n35\nHigh\nCopepod sp.\n50\n\n\nTank_B\n10\n40\n22\n35\nHigh\nCopepod sp.\n50\n\n\nTank_C\n100\n28\n22\n35\nHigh\nCopepod sp.\n50\n\n\nTank_D\n1000\n12\n22\n35\nHigh\nCopepod sp.\n50\n\n\n\nEach row = one tank‚Äôs measurement.\n\n\n\n\n\n\nExample 4: Algal Bloom Effects on Oxygen Levels\n\n\n\n\n\n\nSolution 4 | Click to show\n\n\n\n\n\nTidy Table Example:\n\n\n\n\n\n\n\n\n\n\ndate\nalgal_biomass\ndissolved_oxygen\nwater_temp_C\nnutrient_conc\n\n\n\n\n2024-01-01\n12.5\n6.8\n18.2\n3.1\n\n\n2024-01-08\n15.3\n5.4\n18.4\n3.2\n\n\n2024-01-15\n20.1\n4.7\n18.6\n3.0\n\n\n\nControl variables (temperature and nutrients) are recorded alongside the key variables to control for confounding effects.\n\n\n\n\n\n\nExample 5: Noise Pollution Impact on Dolphin Communication\n\n\n\n\n\n\nSolution 5 | Click to show\n\n\n\n\n\nTidy Table Example:\n\n\n\n\n\n\n\n\n\n\n\n\n\nsession_id\nnoise_level\nwhistle_freq\nwhistle_duration\nrest_period_min\npod_id\nenclosure_size_m\ntime_of_day\n\n\n\n\nSess_1\nLow\n7500\n2.1\n30\nPod_1\n50\nMorning\n\n\nSess_2\nMedium\n7100\n1.8\n30\nPod_1\n50\nMorning\n\n\nSess_3\nHigh\n6800\n1.4\n30\nPod_1\n50\nMorning\n\n\n\nEach row = one noise exposure session.\n\n\n\n\n\n\nGoal | To ‚Äúreverse engineer‚Äù the experimental design and research question from a tidy table to build critical thinking skills.\nInstructions | In this exercise, you are given tidy data tables. Your task:\n\nInterpret: What scientific question might the researchers be asking?\nIdentify Variables: Which variables are:\n\nIndependent (manipulated or compared)\nDependent (measured outcome)\nControl (kept constant or recorded for adjustment)\n\nDescribe Experimental Design: Is it observational, controlled, randomized?\n\n\n\nExample 1\n\n\n\n\n\n\n\n\n\n\n\ntank_id\nlight_intensity\nnutrient_level\ntemp_C\nseaweed_species\nbiomass_increase\n\n\n\n\nTank_1\n100\nLow\n18\nUlva sp.\n4.5\n\n\nTank_2\n100\nHigh\n18\nUlva sp.\n8.1\n\n\nTank_3\n200\nLow\n18\nUlva sp.\n6.2\n\n\nTank_4\n200\nHigh\n18\nUlva sp.\n11.4\n\n\n\n\n\n\n\n\n\nSolution 1 | Click to show\n\n\n\n\n\nPossible Research Question:\n\nHow do light intensity and nutrient levels affect seaweed biomass increase under controlled conditions?\n\nExperimental Design:\nControlled laboratory factorial experiment (2x2 design: light √ó nutrient).\nVariables:\n\nIndependent Variables: light_intensity (100, 200), nutrient_level (Low, High)\nDependent Variable: biomass_increase\nControl Variables: temperature, seaweed_species (constant across tanks)\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\n\n\n\n\n\nsite_id\nfishing_pressure\ndepth_m\nhabitat_type\nsurvey_method\nshark_count\n\n\n\n\nSite_A\nHigh\n25\nReef\nBaited Camera\n3\n\n\nSite_B\nLow\n25\nReef\nBaited Camera\n8\n\n\nSite_C\nHigh\n30\nSandy\nBaited Camera\n2\n\n\nSite_D\nLow\n30\nSandy\nBaited Camera\n6\n\n\n\n\n\n\n\n\n\nSolution 2 | Click to show\n\n\n\n\n\nPossible Research Question:\n\nIs shark abundance lower in areas with high fishing pressure compared to low fishing pressure areas?\n\nExperimental Design:\nObservational comparative study (across sites).\nVariables:\n\nIndependent Variable: fishing_pressure (High, Low)\nDependent Variable: shark_count\nControl Variables: depth_m, habitat_type, survey_method (standardized across sites)\n\n\n\n\n\n\n\nExample 3\n\n\n\n\n\n\n\n\n\n\n\nsample_id\nsalinity\ntemp_C\nsample_depth_m\nchlorophyll_conc\nplankton_sp_count\n\n\n\n\nSample_1\n30\n22\n5\n2.5\n15\n\n\nSample_2\n35\n22\n5\n3.1\n18\n\n\nSample_3\n40\n22\n5\n4.2\n12\n\n\nSample_4\n45\n22\n5\n4.8\n9\n\n\n\n\n\n\n\n\n\nSolution 3 | Click to show\n\n\n\n\n\nPossible Research Question:\n\nHow does salinity influence plankton species diversity in coastal waters?\n\nExperimental Design:\nObservational gradient study.\nVariables:\n\nIndependent Variable: salinity (continuous gradient)\nDependent Variable: plankton_species_count\nControl Variables: temperature, sample_depth_m, chlorophyll_concentration (recorded)\n\n\n\n\n\n\n\n\n1. Experimental Design & Variables\n\nWhat information helps you distinguish between independent, dependent, and control variables?\nHow can you tell whether a study is controlled, randomized, or observational based solely on the dataset?\nWhy is it important to clearly document control variables, even if they are kept constant?\nIn some examples, more than one independent variable is manipulated. How would you recognize and interpret this factorial design?\nHow might confounding variables influence the results if not properly controlled?\n\n\n\n\n2. Tidy Data Structure\n\nWhy is it useful to structure experimental data in a tidy format (one variable per column, one observation per row)?\nWhat could go wrong in analysis or visualization if data were not kept in tidy format?\nHow do identifiers like tank_id, site_id, or session_id help maintain data clarity and traceability?\n\n\n\n\n3. Critical Interpretation\n\nCould there be alternative research questions addressed with the same dataset? If so, how?\nAre there any additional variables you might want to collect to strengthen the experimental design or account for potential confounders?\nIf you encountered missing or inconsistent data in these tables, how would it affect your interpretation?\nHow do replication and sample size considerations appear (or not appear) in tidy datasets like these?\nHow could these tables be prepared for statistical modeling (e.g., regression analysis)?\n\n\n\n\n4. Future Application\n\nIn your own research, how would you apply what you‚Äôve learned about tidy data and variable types to plan data collection?\nWhat challenges might arise when moving from narrative descriptions of experiments to data analysis-ready datasets in real-world projects?\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "Week 4 | 3. Exploratory analysis",
    "section": "",
    "text": "Class Details\n\n\n\nüìÖ Date: 01 April, 2025\n‚è∞ Time: 09:30h - 11:30h (+2h) üìñ Synopsis: Synopsis: Organized data analysis project with a clear folder structure, set up an RStudio project with Git, and connected it to GitHub for version control and collaboration.\n\n\n\n\nDone in ClassTo Do at Home\n\n\n\nNow my data is tidy. What to do next?\n\n\n\n\nTo keep your files organized and easy to find, it‚Äôs a good idea to set up a clear naming system before you start collecting data. A file naming convention is a consistent way to name files so it‚Äôs clear what‚Äôs inside and how each file connects to others. This helps you quickly understand and locate files, avoiding confusion or lost data later on. | Image: https://xkcd.com/1459/\n\n\n\nCreate a directory for the task of writing a data descriptor paper.\n\nA key part of good data management is organizing your data effectively. This means planning how you‚Äôll name files, arrange folders, and show relationships between them.\nResearchers should set up folder structures that reflect how records were created and align with their workflows. Doing this improves clarity, makes it easier to save, find, and archive files, and supports collaboration across teams.\nEstablishing a clear file structure and naming system before collecting data ensures consistency and helps team members work more efficiently together.\nWe propose the following structure:\ndata_descriptor_paper/    \n‚îú‚îÄ‚îÄ data/    \n‚îÇ ‚îú‚îÄ‚îÄ raw/    \n‚îÇ ‚îî‚îÄ‚îÄ processed/   \n‚îú‚îÄ‚îÄ scripts/    \n‚îú‚îÄ‚îÄ output/    \n‚îî‚îÄ‚îÄ sandbox/   \n\ndata_descriptor_paper: root folder for the Rproject\n\ndata/raw: original, unmodified data\n\ndata/processed: cleaned or transformed data ready for analysis\n\nscripts: code for processing and analysis\n\noutput: figures, tables, and final results\n\nsandbox: exploratory or temporary work (to be git ignored)\n\n\nCreate an RStudio project\n\nCreate a new RStudio project in the root folder (data_descriptor_paper).\nIf available, check the option to initialize a Git repository when creating the project.\n\nIf Git was not initialized\n\nIf you didn‚Äôt initialize Git during project setup, open the Terminal tab in RStudio and run: git init\n\nCreate a GitHub repository\n\n\nChoose a short, informative name specific to your project.\nCreate an empty repository (no README, no license).\nSet it to private, especially if working with sensitive data.\n\nA simple Git overview is available here:\nhttps://github.com/iduarte/git-tutorial/blob/master/GitHub-Workshop.md\n\nConnect your local Git repository to GitHub\n\nTo push/pull your local repo to GitHub you must set up a Personal Access Token (PAT) or configure SSH keys for authentication.\n\nGuide to add a PAT to your GitHub account: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\nGuide to add SSH keys to your GitHub account:\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?platform=windows\n\n\n\nBefore the next class, please complete the following tasks:\n\nCreate a metadata file that describes each variable in your tidy dataset.\nDecide whether to use plain R scripts or R Markdown files for your analysis.\nNote: R Markdown (.Rmd) is an example of literate programming - a technique that combines code with explanatory text. It allows both analysis and documentation within a single file, making it a popular format for reproducible research.\nOnce you‚Äôve chosen your format, complete the remaining tasks using that file type. Be sure to annotate your code clearly and commit changes regularly to your local Git repository.\nSummarize your dataset with descriptive statistics.\nPropose effective ways to present and visualize this information. Would tables or plots work best? What approaches do other papers use?\nPerform data quality control and validate the experimental design.\nUse Principal Component Analysis (PCA) to identify major sources of variation.\n\nWhat is PCA and why is it useful?\nCan categorical variables be used in PCA? If so, how? Is numeric encoding appropriate?\nHow should tidy data be formatted for PCA in R?\nWhat additional quality control or validation strategies are used in published work?\n\nExamine univariate distributions of all variables.\n\nWhich plot types are best for each variable type?\nAre there innovative visualization methods to consider?\nWhat do published studies use?\n\nVisualize pairwise (bivariate) relationships between variables.\n\nHow can all variable pairs be visualized in a space efficient way, that is both informative and pleasant?\nIs this approach informative? Why or why not?\n\nDownload the Scientific Data Descriptor template.\n\nVisit: https://www.nature.com/documents/scidata-data-descriptor-document-template.docx\n\nReview the template.\nThe purpose here is to understand a data descriptor structure and the type of information expected in each section.\n\nSelect a Scientific Data Descriptor paper to read and discuss in the next class.\n\nGo to: https://www.nature.com/sdata and find a recent paper related to your area of interest.\n\nRead it critically and prepare a 5-minute summary to present in the next class.\nThe purpose here is to learn by example: What do other papers reporting data look like?\n\n\n\n\n\n\n\n\n Back to top"
  }
]